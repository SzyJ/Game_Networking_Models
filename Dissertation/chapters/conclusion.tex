\chapter{Evaluation and Findings}



\section{Testing my implementation}
In order to keep tests fair, it is important to keep as much of the environment constant as possible between test iterations. Each iteration should have one variable that is changed so that the effects of this change can be easily observed and analysed. The following sections will outline the tests on how changing the volume of traffic that is sent between clients on a consumer network, effects the jitter with which the messages arrive.

The test is set up using the client hosted model where the clients are connected to the network on both an ethernet connection and WiFi. The host is connected to the router over ethernet cables that go through a switch. One client that will connect to the host is started on the same machine as the server; this client will be refered to as the ``localhost client''. Another client is connected directly to the router and it will be refered to as the ``ethernet client''. The last client that will connect to the host, is a laptop that is close to the router and connected over WiFi, this will be refered to as the ``WiFi client''.

Once the connection has been established, the server tickrate can be configured. This will be the variable that will be changed and the effects of this will be observed at each stage of the experiment. To minimise any external factors that could effect this experiment, all tests were performed at night when the network is under least stress.

\subsection{Capturing and analysing the results}
When writing the GNAT library, one of the most important features was the custom wrapper for a logging library that would go on to be used extensively during the development and testing. This wrapper allows for a message to be written to a log file from anywhere in the program. The library code is written in such a way that any action that is done, is logged under different logging levels depending on the nature of the information. One piece of information that is logged whenever a message is received by a client, is the time that has elapsed since the last message that has been received. This information can then in turn be used to calculate the jitter of incomming messages and this is what is used to generate data in the following experiments.

A simple script was used to extract the values, on each log file (one for each client and therefore three for each test). The script would use RegEx to look for lines where the jitter information is logged and extract the number from the rest of the line. The output would be a file with 1 number per line which can then be fed into another program to generate graphs. The analysis of this data and some examples of the graphs produced in this way, can be found under the section \ref{sec:jitter_results}.

In order to compare how varied the numbers were from one another, each result set, was fed as input into a math function calculating the variance ($\sigma^2$) of the data. The analysis of this can be found in the section \ref{sec:variance_results}.

\newpage
\subsection{Jitter between WiFi and Ethernet connections} \label{sec:jitter_results}
My expectation for the results from this test, were that more anomolous values would be present in WiFi packets rather than Ethernet packets due to the higher chance of packet loss and less reliable connection technology. The jitter in WiFi data is also expected to be more varied than a wired connection.
After harvesting and extracting the data from my experiment, I have ended up with over 5000 numbers for each client and each test. Each number represents the time between two received messages arriving and so it is understandable that if a given packet takes longer then average to reach the client, a larger number will be produced however as a result when the next packet is received (after traveling for what can be assumed to be an average time), it is likely that the next value will be smaller than average. This nature, produces the zig-zag like shape that can be seen in the graphs where if there is a large spike in one direction, it is likely that the next data value, will then produce a large spike in the other direction. The graphs representing this data can be seen in figures \ref{fig:250ms_graph}, \ref{fig:64ms_graph} and \ref{fig:8ms_graph}. These graphs show the data for the WiFi client and the Ethernet client for 250ms, 64ms and 8ms server tick-rates respectively and each one has been limited to use the same amount of data to make the horisontal axis scale consistant. More graphs of this data, including data for different tickrates, can be found in the appendix.

For each dataset, the data turned out to be very similar; it can be seen that in general the data for the Ethernet client, arrived within 1 millisecond, however as expected, the data for the WiFi client appeared to be slightly more varied with most of the data arriving within 2 milliseconds of the average. Another anomaly that can be seen with the WiFi connection throughout each test, is that it is much more common for a ``random'' spike to occur. These spikes could occur due to several reasons. One example is that packet loss occured causing the wait for the next packet to essentially double. This can be seen in several instances through one value having an abnormally large value but not followed by an abnormally small value directly after.

An effect that can be observed with the different tick-rates, is that it is much less common for abnormally large or small, anomalous data to be seen in WiFi connections as the tick-rate gets faster. As the tick-rate increases, each anomalous result (whether it is due to packet loss or suboptimal routing) is much less impactful if there is another packet arriving soon after. An exception to this rule however can be seen in the 8ms test. The packets in the data for the 8ms test appear much more varied in general more anomalous spikes can be seen in both WiFi and Ethernet data (despite this trend being more obvious in WiFi data). To show these anomalous spikes, I have made the scale 2.5ms per horisontal line instead of 1ms like it is on the other graphs. Unfortunately, it is unclear why this exception occurs in this test when in every other test, the varience can be seen to be getting smaller.

\newpage
\input{fixtures/result_graphs}


\subsection{Calculating the variance in jitter}\label{sec:variance_results}
The next question to anwser was: Objectively, which collection of gathered data, had the least variance in it? This data can be seen represented in a bar chart in figure \ref{fig:jitter_variance}. The $\sigma^2$ values for WiFi at the tickrate of 500 and 1000 milliseconds, were much larger than the other results due to having to wait for a much longer time if a packet was lost. Even when not considering these anomalous values however, it can be seen that the varience in the receiving rate is on average 30\% larger than the average of ethernet and localhost data.

One unexpetced result was that even the localhost client experianced a certain level of jitter in all tests. This is likely be due to the operating system prioritising certain programs over others as well how the NIC hardware and winsock manages sending and receiving of data.

Another surprising result was once again seen at the 8ms test. Following on from the previous section, it can be seen that the variance of the data incereases when compared to the other results despite what would intuitively be the most ``reliable'' constant stream of data. The fact that the rise can be seen in all three clients, indicates that there may be some external factor (such as my computer's NIC or consumer grade router hardware), that interferes with these results. In general however, it is unlikely to see broadcast rates higher than 63Hz (around 16ms delay) anyway.

\input{fixtures/variance_graph}

\chapter{Conclusion}
This chapter presents a conclusion of the project based on the aims and objectives that were outline in section \ref{sec:aims_and_objectives}. The following sections will discuss the projects success by determining if each objective has been met and as a result, if the aim of the project has been met.

\section{Satisfaction of the Aims and Objectives}
\begin{enumerate}
\item Investigate the common practices and techniques used in modern multiplayer games
  \\\\
  Extensive research into what kind of netcode different examples of popular, modern games use has been done and network development tools in the Unity game engine have been tested through real-world use. The nature of choosing a networking model is a choice that rarely has a concrete single anwser. It is often the case that within the same game instance, multiple different networking models are used for synchronising different aspects of the simulation. For example, one of the players might be hosting the physics server for other players, but the rest of the simulation is synchronised through the peer to peer model instead, taking advantage of faster communication between any two players but having the game physics synchronised between all clients through a single instance.


\item Implement the peer to peer and client hosted networking models as a C++ library.
  \\\\
  TODO

\item Investigate issues spawned from computer network communication using the implemented networking models in the GNAT library.
  \\\\
  TODO
\end{enumerate}





\section{What has been established and what could be done further}
TODO


\section{What went well}
TODO


\section{What could have been done better}
TODO


\section{What can be done in the future}
TODO
